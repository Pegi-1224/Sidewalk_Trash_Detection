{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888aeeab-299a-4616-aaac-fb58f5f19e8a",
   "metadata": {},
   "source": [
    "## Read YOLO format laebl, and define function to train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d8a8ee-6ac3-4ee9-a553-6c7b97c98aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# Dataset class for YOLO format\n",
    "class YOLODetectionDataset(Dataset):\n",
    "    def __init__(self, image_dirs, label_dirs, transform=None):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for img_dir, lbl_dir in zip(image_dirs, label_dirs):\n",
    "            for fname in os.listdir(img_dir):\n",
    "                if not fname.lower().endswith(('.jpg','jpeg','.png')):\n",
    "                    continue\n",
    "                img_path = os.path.join(img_dir, fname)\n",
    "                lbl_path = os.path.join(lbl_dir, os.path.splitext(fname)[0] + '.txt')\n",
    "                \n",
    "                # only keep if .txt exists _and_ is nonâ€‘empty\n",
    "                if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(lbl_path)\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        boxes, labels = [], []\n",
    "        with open(self.labels[idx]) as f:\n",
    "            for line in f:\n",
    "                cls, x_c, y_c, w, h = map(float, line.split())\n",
    "                ih, iw = img.shape[1], img.shape[2]\n",
    "                x_c, y_c, w, h = x_c*iw, y_c*ih, w*iw, h*ih\n",
    "                x_min = x_c - w/2; y_min = y_c - h/2\n",
    "                x_max = x_c + w/2; y_max = y_c + h/2\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(1)  # Only one class\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "        return img, target\n",
    "\n",
    "# Utility functions\n",
    "def collate_fn(batch): return tuple(zip(*batch))\n",
    "\n",
    "def train_model(loader, model, optimizer, device, epochs=10):\n",
    "    model.to(device).train()\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, targets in loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targs = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targs)\n",
    "            loss = sum(loss_dict.values())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {e+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluation (built it same as the YOLO)\n",
    "def evaluate_model(loader, model, device, conf_thr=0.001):\n",
    "    model.to(device).eval()\n",
    "    # IoU thresholds 0.50:0.95\n",
    "    mAP_default = MeanAveragePrecision()\n",
    "    # Only IoU = 0.50\n",
    "    mAP50 = MeanAveragePrecision(iou_thresholds=[0.5])\n",
    "\n",
    "    n_images = 0\n",
    "    n_instances = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            # count images & GT boxes\n",
    "            n_images    += len(images)\n",
    "            n_instances += sum(t[\"boxes\"].shape[0] for t in targets)\n",
    "\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            preds_default, preds_50, gts = [], [], []\n",
    "            for out, tgt in zip(outputs, targets):\n",
    "                # apply a confidence threshold\n",
    "                keep = out[\"scores\"] >= conf_thr\n",
    "\n",
    "                boxes  = out[\"boxes\"][keep].cpu()\n",
    "                scores = out[\"scores\"][keep].cpu()\n",
    "                labels = out[\"labels\"][keep].cpu()\n",
    "\n",
    "                pred = {\"boxes\": boxes, \"scores\": scores, \"labels\": labels}\n",
    "                preds_default.append(pred)\n",
    "                preds_50.append(pred)\n",
    "\n",
    "                gts.append({\"boxes\": tgt[\"boxes\"], \"labels\": tgt[\"labels\"]})\n",
    "\n",
    "            # update both metrics\n",
    "            mAP_default.update(preds_default, gts)\n",
    "            mAP50.update(preds_50, gts)\n",
    "\n",
    "    res_def = mAP_default.compute()\n",
    "    res_50  = mAP50.compute()\n",
    "    P = res_50[\"map\"].item()\n",
    "    R = res_50[\"mar_100\"].item()\n",
    "    mAP50v = res_50[\"map\"].item()\n",
    "    mAP5095v = res_def[\"map\"].item()\n",
    "\n",
    "    header = f\"{'Class':<10}{'Images':>8}{'Instances':>12}{'Box(P)':>8}{'R':>8}{'mAP50':>8}{'mAP50-95':>12}\"\n",
    "    row = f\"{'all':<10}{n_images:8d}{n_instances:12d}{P:8.3f}{R:8.3f}{mAP50v:8.3f}{mAP5095v:12.3f}\"\n",
    "    print(header)\n",
    "    print(row)\n",
    "\n",
    "    return {\n",
    "        \"Class\": \"all\",\n",
    "        \"Images\": n_images,\n",
    "        \"Instances\": n_instances,\n",
    "        \"P\": P,\n",
    "        \"R\": R,\n",
    "        \"mAP50\": mAP50v,\n",
    "        \"mAP50-95\": mAP5095v\n",
    "    }\n",
    "\n",
    "# Paths\n",
    "orig_img_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\roboflow_trash_dataset\\train\\images\"\n",
    "orig_lbl_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\roboflow_trash_dataset\\train\\labels\"\n",
    "val_img_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\roboflow_trash_dataset\\valid\\images\"\n",
    "val_lbl_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\roboflow_trash_dataset\\valid\\labels\"\n",
    "ai_img_dir = r\"C:\\Users\\peggy\\generated_trash_images\\vott-json-export\\images\"\n",
    "ai_lbl_dir = r\"C:\\Users\\peggy\\generated_trash_images\\vott-json-export\\labels\"\n",
    "\n",
    "save_dir = r\"C:\\Users\\peggy\\OneDrive\\Desktop\\Trash_detection\\models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0c414-2163-42a9-b280-52c4fead5ee1",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f726c65-409a-40c5-bfab-412ead4aa5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training on original data ===\n",
      "Epoch 1/30, Loss: 24.7681\n",
      "Epoch 2/30, Loss: 19.5854\n",
      "Epoch 3/30, Loss: 18.0669\n",
      "Epoch 4/30, Loss: 16.7234\n",
      "Epoch 5/30, Loss: 16.1960\n",
      "Epoch 6/30, Loss: 15.4970\n",
      "Epoch 7/30, Loss: 14.8551\n",
      "Epoch 8/30, Loss: 14.7996\n",
      "Epoch 9/30, Loss: 14.1542\n",
      "Epoch 10/30, Loss: 13.7943\n",
      "Epoch 11/30, Loss: 13.0350\n",
      "Epoch 12/30, Loss: 12.5883\n",
      "Epoch 13/30, Loss: 11.9334\n",
      "Epoch 14/30, Loss: 11.8084\n",
      "Epoch 15/30, Loss: 10.9434\n",
      "Epoch 16/30, Loss: 10.5290\n",
      "Epoch 17/30, Loss: 9.7844\n",
      "Epoch 18/30, Loss: 11.0196\n",
      "Epoch 19/30, Loss: 9.7792\n",
      "Epoch 20/30, Loss: 9.1784\n",
      "Epoch 21/30, Loss: 8.8238\n",
      "Epoch 22/30, Loss: 8.6329\n",
      "Epoch 23/30, Loss: 8.3715\n",
      "Epoch 24/30, Loss: 7.9129\n",
      "Epoch 25/30, Loss: 7.9295\n",
      "Epoch 26/30, Loss: 7.2794\n",
      "Epoch 27/30, Loss: 7.3696\n",
      "Epoch 28/30, Loss: 6.7385\n",
      "Epoch 29/30, Loss: 6.6272\n",
      "Epoch 30/30, Loss: 6.6839\n",
      "=== Evaluating original model ===\n",
      "Class       Images   Instances  Box(P)       R   mAP50    mAP50-95\n",
      "all            244         505   0.603   0.733   0.603       0.344\n",
      "Original data training took 1158.29 seconds (19.30 minutes)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_ds1 = YOLODetectionDataset([orig_img_dir],[orig_lbl_dir])\n",
    "train_loader1 = DataLoader(train_ds1, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_ds    = YOLODetectionDataset([val_img_dir],[val_lbl_dir])\n",
    "val_loader= DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model1 = fasterrcnn_resnet50_fpn(weights = None, num_classes=2)\n",
    "opt1   = torch.optim.SGD(model1.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "torch.save(model1.state_dict(), os.path.join(save_dir, \"fasterrcnn_original.pth\"))\n",
    "\n",
    "\n",
    "print(\"=== Training on original data ===\")\n",
    "start_time = time.time()\n",
    "train_model(train_loader1, model1, opt1, device, epochs=30)\n",
    "end_time = time.time()\n",
    "print(\"=== Evaluating original model ===\")\n",
    "evaluate_model(val_loader, model1, device)\n",
    "\n",
    "orig_duration = end_time - start_time\n",
    "print(f\"Original data training took {orig_duration:.2f} seconds ({orig_duration/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf86ab-96b4-48ff-a125-e933361117c1",
   "metadata": {},
   "source": [
    "## Conclusion on the original dataset\n",
    "Parameter: epochs=30, batch=16\n",
    "\n",
    "## Result\n",
    "number of image: 244\n",
    "\n",
    "Precision: 0.603\n",
    "\n",
    "Recall: 0.733\n",
    "\n",
    "mAP@50: 0.603 \n",
    "\n",
    "mAP@50-95: 0.344\n",
    "\n",
    "## Time used\n",
    "Training: 1158.29 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877f8ca-ff30-4616-9b3c-4703a6c9b1df",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "888a2dd2-5952-41b9-b339-64a01defad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(save_dir, \"fasterrcnn_trained.pth\")\n",
    "torch.save(model1.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaa3ec-d3a8-4931-af26-da5ec368b445",
   "metadata": {},
   "source": [
    "### Test using my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109b0cc2-d0f3-4bb0-bbcb-210770a7b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "def evaluate_fasterrcnn(img_dir,lbl_dir, checkpoint_path, num_classes = 2, batch_size = 1, conf_thr = 0.001, device = \"cuda\"):\n",
    "    # Build & load model\n",
    "    model = fasterrcnn_resnet50_fpn(weights=None, num_classes=num_classes)\n",
    "    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare dataLoader\n",
    "    test_ds = YOLODetectionDataset([img_dir], [lbl_dir])\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size,\n",
    "                             shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    start = time.time()\n",
    "    results = evaluate_model(test_loader, model, device, conf_thr)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    num_images = results[\"Images\"]\n",
    "    avg_per_image = elapsed / num_images\n",
    "    \n",
    "    print(f\"Total evaluation time: {elapsed:.2f}s\")\n",
    "    print(f\"Avg time per image: {avg_per_image:.4f}s ({avg_per_image*1000:.1f} ms)\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d3f11c2-9be0-4fb6-813d-8e42fdd69bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peggy\\AppData\\Local\\Temp\\ipykernel_7488\\1850192063.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class       Images   Instances  Box(P)       R   mAP50    mAP50-95\n",
      "all            185         253   0.699   0.862   0.699       0.392\n",
      "Total evaluation time: 57.56s\n",
      "Avg time per image: 0.3111s (311.1 ms)\n"
     ]
    }
   ],
   "source": [
    "metrics2 = evaluate_fasterrcnn(\n",
    "img_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\my_trash_dataset\\images\",\n",
    "lbl_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\my_trash_dataset\\labels\",\n",
    "checkpoint_path = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\models\\fasterrcnn_trained.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8f436-f7de-4441-9f97-a369c0ed1190",
   "metadata": {},
   "source": [
    "## Result on my data\n",
    "Precision: 0.699\n",
    "\n",
    "Recall: 0.862\n",
    "\n",
    "mAP50: 0.699\n",
    "\n",
    "mAP50-95: 0.392\n",
    "\n",
    "Speed: Avg inference time/image: 311.1 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916f121-a854-4053-b309-619deb94cc7d",
   "metadata": {},
   "source": [
    "## Train with AI generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac52e4f-dfec-4aef-946b-5d3ebbcabac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training on merged data ===\n",
      "Epoch 1/30, Loss: 30.5740\n",
      "Epoch 2/30, Loss: 24.9909\n",
      "Epoch 3/30, Loss: 22.2866\n",
      "Epoch 4/30, Loss: 20.2479\n",
      "Epoch 5/30, Loss: 19.5244\n",
      "Epoch 6/30, Loss: 18.8146\n",
      "Epoch 7/30, Loss: 18.0510\n",
      "Epoch 8/30, Loss: 17.1518\n",
      "Epoch 9/30, Loss: 16.7073\n",
      "Epoch 10/30, Loss: 15.6495\n",
      "Epoch 11/30, Loss: 14.7425\n",
      "Epoch 12/30, Loss: 14.1078\n",
      "Epoch 13/30, Loss: 13.7980\n",
      "Epoch 14/30, Loss: 12.8606\n",
      "Epoch 15/30, Loss: 12.2356\n",
      "Epoch 16/30, Loss: 11.3058\n",
      "Epoch 17/30, Loss: 11.1532\n",
      "Epoch 18/30, Loss: 10.7390\n",
      "Epoch 19/30, Loss: 10.1994\n",
      "Epoch 20/30, Loss: 9.9895\n",
      "Epoch 21/30, Loss: 9.1777\n",
      "Epoch 22/30, Loss: 8.9881\n",
      "Epoch 23/30, Loss: 8.9582\n",
      "Epoch 24/30, Loss: 8.1195\n",
      "Epoch 25/30, Loss: 7.8011\n",
      "Epoch 26/30, Loss: 7.7971\n",
      "Epoch 27/30, Loss: 7.7635\n",
      "Epoch 28/30, Loss: 7.2568\n",
      "Epoch 29/30, Loss: 7.2879\n",
      "Epoch 30/30, Loss: 7.1140\n",
      "=== Evaluating merged model ===\n",
      "Class       Images   Instances  Box(P)       R   mAP50    mAP50-95\n",
      "all            244         505   0.602   0.689   0.602       0.346\n",
      "Merged data training took 1869.89 seconds (31.16 minutes)\n"
     ]
    }
   ],
   "source": [
    "train_ds2 = YOLODetectionDataset([orig_img_dir, ai_img_dir],\n",
    "                                 [orig_lbl_dir, ai_lbl_dir])\n",
    "train_loader2 = DataLoader(train_ds2, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model2 = fasterrcnn_resnet50_fpn(weights=None, num_classes=2)\n",
    "opt2   = torch.optim.SGD(model2.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "torch.save(model2.state_dict(), os.path.join(save_dir, \"fasterrcnn_merged.pth\"))\n",
    "\n",
    "print(\"=== Training on merged data ===\")\n",
    "start_time = time.time()\n",
    "train_model(train_loader2, model2, opt2, device, epochs=30)\n",
    "end_time = time.time()\n",
    "print(\"=== Evaluating merged model ===\")\n",
    "evaluate_model(val_loader, model2, device)\n",
    "\n",
    "merged_duration = end_time - start_time\n",
    "print(f\"Merged data training took {merged_duration:.2f} seconds ({merged_duration/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9d97f-8af5-4728-a751-2a41280da1f4",
   "metadata": {},
   "source": [
    "## Conclusion on the merged dataset\n",
    "Parameter: epochs=200, imgsz=640, batch=16\n",
    "\n",
    "## Result\n",
    "number of image: 244\n",
    "\n",
    "Precision: 0.602\n",
    "\n",
    "Recall: 0.689\n",
    "\n",
    "mAP@50: 0.602 \n",
    "\n",
    "mAP@50-95: 0.346\n",
    "\n",
    "## Time used\n",
    "Training: 1869.89 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd5931-e7d1-4fe8-b4fe-706b51a72380",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "921da75c-0d38-4592-b5b7-1a4cda7e6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(save_dir, \"fasterrcnn_trained_AI.pth\")\n",
    "torch.save(model2.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20bcf7-2b9b-4a5d-84db-78ffee29d94b",
   "metadata": {},
   "source": [
    "### Test with my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8d3e765-7c49-4ff7-9abd-f6a617a589c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peggy\\AppData\\Local\\Temp\\ipykernel_7488\\1850192063.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class       Images   Instances  Box(P)       R   mAP50    mAP50-95\n",
      "all            185         253   0.700   0.850   0.700       0.402\n",
      "Total evaluation time: 58.79s\n",
      "Avg time per image: 0.3178s (317.8 ms)\n"
     ]
    }
   ],
   "source": [
    "metrics2 = evaluate_fasterrcnn(\n",
    "img_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\my_trash_dataset\\images\",\n",
    "lbl_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\my_trash_dataset\\labels\",\n",
    "checkpoint_path = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\models\\fasterrcnn_trained_AI.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3cb0f9-0e0c-405d-a5d5-b8bc383e7c8c",
   "metadata": {},
   "source": [
    "## Result on my data\n",
    "Precision: 0.700\n",
    "\n",
    "Recall: 0.850\n",
    "\n",
    "mAP50: 0.700\n",
    "\n",
    "mAP50-95: 0.402\n",
    "\n",
    "Speed: Avg inference time/image: 317.8 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cffd97-e863-4b32-bfa4-142b91cf6877",
   "metadata": {},
   "source": [
    "## Data argumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75f393e1-2ea6-469d-b2f8-cba1f5c4014f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training on improved model ===\n",
      "Epoch 1/30, Loss: 129.5551\n",
      "Epoch 2/30, Loss: 100.9766\n",
      "Epoch 3/30, Loss: 92.9542\n",
      "Epoch 4/30, Loss: 86.0217\n",
      "Epoch 5/30, Loss: 92.4837\n",
      "Epoch 6/30, Loss: 86.6239\n",
      "Epoch 7/30, Loss: 84.6484\n",
      "Epoch 8/30, Loss: 85.3347\n",
      "Epoch 9/30, Loss: 84.8122\n",
      "Epoch 10/30, Loss: 76.0549\n",
      "Epoch 11/30, Loss: 74.8603\n",
      "Epoch 12/30, Loss: 75.7057\n",
      "Epoch 13/30, Loss: 71.6443\n",
      "Epoch 14/30, Loss: 70.4226\n",
      "Epoch 15/30, Loss: 68.1486\n",
      "Epoch 16/30, Loss: 73.6413\n",
      "Epoch 17/30, Loss: 65.0139\n",
      "Epoch 18/30, Loss: 62.2270\n",
      "Epoch 19/30, Loss: 62.9046\n",
      "Epoch 20/30, Loss: 62.0147\n",
      "Epoch 21/30, Loss: 65.1031\n",
      "Epoch 22/30, Loss: 67.6373\n",
      "Epoch 23/30, Loss: 61.1340\n",
      "Epoch 24/30, Loss: 61.9004\n",
      "Epoch 25/30, Loss: 57.8253\n",
      "Epoch 26/30, Loss: 55.4636\n",
      "Epoch 27/30, Loss: 53.1573\n",
      "Epoch 28/30, Loss: 59.9789\n",
      "Epoch 29/30, Loss: 61.0484\n",
      "Epoch 30/30, Loss: 57.5018\n",
      "=== Evaluating on improved model ===\n",
      "Class       Images   Instances  Box(P)       R   mAP50    mAP50-95\n",
      "all            244         472   0.620   0.739   0.620       0.379\n",
      "Improved data training took 1398.30 seconds (23.31 minutes)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, RandomBrightnessContrast, Resize, Normalize,\n",
    "    HueSaturationValue, ShiftScaleRotate, BboxParams, RandomScale, RandomCrop\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Define transforms\n",
    "train_transforms = Compose([\n",
    "    RandomScale(scale_limit=(0.5, 2.0), p=0.7),\n",
    "    RandomCrop(600, 600, p=0.5),\n",
    "    Resize(800, 800, p=1.0),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    RandomBrightnessContrast(p=0.5),\n",
    "    HueSaturationValue(p=0.5),\n",
    "    Normalize(mean=[0.0, 0.0, 0.0], std=[255.0, 255.0, 255.0], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "], \n",
    "bbox_params=BboxParams(format='pascal_voc', label_fields=['labels'], min_area=0.0, min_visibility=0.0))\n",
    "\n",
    "val_transforms = Compose([\n",
    "    Normalize(mean=[0.0, 0.0, 0.0], std=[255.0, 255.0, 255.0], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "], \n",
    "bbox_params=BboxParams(format='pascal_voc', label_fields=['labels'], min_area=0.0, min_visibility=0.0))\n",
    "\n",
    "# New dataset class for augmented data\n",
    "class AugmentedRCNNDataset(Dataset):\n",
    "    def __init__(self, img_dir, lbl_dir, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.lbl_dir = lbl_dir\n",
    "        self.transforms = transforms\n",
    "        self.images = []\n",
    "        # Collect valid images with non-empty labels\n",
    "        for fname in os.listdir(img_dir):\n",
    "            if not fname.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "                continue\n",
    "            lbl_path = os.path.join(lbl_dir, os.path.splitext(fname)[0] + '.txt')\n",
    "            if os.path.exists(lbl_path) and os.path.getsize(lbl_path) > 0:\n",
    "                self.images.append(fname)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "        lbl_path = os.path.join(self.lbl_dir, os.path.splitext(fname)[0] + '.txt')\n",
    "\n",
    "        # Load original image and boxes\n",
    "        img_np = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        H_orig, W_orig = img_np.shape[:2]\n",
    "        boxes_orig, labels_orig = [], []\n",
    "        with open(lbl_path) as f:\n",
    "            for line in f:\n",
    "                cls, x_c, y_c, w, h = map(float, line.strip().split())\n",
    "                # Convert YOLO format to Pascal VOC\n",
    "                x_c *= W_orig\n",
    "                y_c *= H_orig\n",
    "                w *= W_orig\n",
    "                h *= H_orig\n",
    "                x0 = x_c - w/2\n",
    "                y0 = y_c - h/2\n",
    "                x1 = x_c + w/2\n",
    "                y1 = y_c + h/2\n",
    "                boxes_orig.append([x0, y0, x1, y1])\n",
    "                labels_orig.append(1)  # Single class\n",
    "\n",
    "        boxes, labels = boxes_orig.copy(), labels_orig.copy()\n",
    "        img_transformed = img_np.copy()\n",
    "        applied_augmentation = False\n",
    "\n",
    "        if self.transforms:\n",
    "            for _ in range(3):  # Retry augmentation up to 3 times\n",
    "                # Apply augmentation\n",
    "                try:\n",
    "                    augmented = self.transforms(\n",
    "                        image=img_np,\n",
    "                        bboxes=boxes_orig,\n",
    "                        labels=labels_orig\n",
    "                    )\n",
    "                    img_transformed = augmented['image']\n",
    "                    boxes_aug = augmented['bboxes']\n",
    "                    labels_aug = augmented['labels']\n",
    "                    H, W = img_transformed.shape[:2]\n",
    "\n",
    "                    # Process boxes: clamp and validate\n",
    "                    valid_boxes = []\n",
    "                    valid_labels = []\n",
    "                    for box, lbl in zip(boxes_aug, labels_aug):\n",
    "                        x0, y0, x1, y1 = box\n",
    "                        # Clamp to image dimensions\n",
    "                        x0 = max(0, min(x0, W))\n",
    "                        x1 = max(0, min(x1, W))\n",
    "                        y0 = max(0, min(y0, H))\n",
    "                        y1 = max(0, min(y1, H))\n",
    "                        # Check area\n",
    "                        if x1 > x0 and y1 > y0:\n",
    "                            valid_boxes.append([x0, y0, x1, y1])\n",
    "                            valid_labels.append(lbl)\n",
    "                    \n",
    "                    if len(valid_boxes) > 0:\n",
    "                        boxes = valid_boxes\n",
    "                        labels = valid_labels\n",
    "                        applied_augmentation = True\n",
    "                        break\n",
    "                except:\n",
    "                    continue  # Retry on augmentation error\n",
    "\n",
    "            if not applied_augmentation:\n",
    "                # Fallback to original image with valid boxes\n",
    "                H, W = H_orig, W_orig\n",
    "                valid_boxes = []\n",
    "                valid_labels = []\n",
    "                for box, lbl in zip(boxes_orig, labels_orig):\n",
    "                    x0, y0, x1, y1 = box\n",
    "                    x0 = max(0, min(x0, W))\n",
    "                    x1 = max(0, min(x1, W))\n",
    "                    y0 = max(0, min(y0, H))\n",
    "                    y1 = max(0, min(y1, H))\n",
    "                    if x1 > x0 and y1 > y0:\n",
    "                        valid_boxes.append([x0, y0, x1, y1])\n",
    "                        valid_labels.append(lbl)\n",
    "                boxes = valid_boxes\n",
    "                labels = valid_labels\n",
    "                # Apply normalization and ToTensor manually\n",
    "                img_transformed = img_np.astype(np.float32) / 255.0\n",
    "                img_transformed = torch.from_numpy(img_transformed).permute(2, 0, 1)\n",
    "        else:\n",
    "            # No transforms applied\n",
    "            H, W = H_orig, W_orig\n",
    "            img_transformed = torch.from_numpy(img_np).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Final check for valid boxes\n",
    "        if len(boxes) == 0:\n",
    "            raise ValueError(f\"No valid boxes for {fname} after processing.\")\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "        return img_transformed, target\n",
    "\n",
    "\n",
    "# Instantiate model (same as before)\n",
    "model = fasterrcnn_resnet50_fpn(weights=None, num_classes=2)\n",
    "in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes=2)\n",
    "\n",
    "# Build dataLoaders \n",
    "train_ds = AugmentedRCNNDataset(\n",
    "    img_dir=orig_img_dir,\n",
    "    lbl_dir=orig_lbl_dir,\n",
    "    transforms=train_transforms\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_ds = AugmentedRCNNDataset(\n",
    "    img_dir=val_img_dir,\n",
    "    lbl_dir=val_lbl_dir,\n",
    "    transforms=val_transforms\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Train & evaluate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "print(\"=== Training on improved model ===\")\n",
    "start_time = time.time()\n",
    "train_model(train_loader, model, optimizer, device, epochs=30)\n",
    "end_time = time.time()\n",
    "print(\"=== Evaluating on improved model ===\")\n",
    "evaluate_model(val_loader, model, device)\n",
    "print(f\"Improved data training took {orig_duration:.2f} seconds ({orig_duration/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bcea3-4703-4122-8e74-db78078d83f1",
   "metadata": {},
   "source": [
    "## Conclusion on the merged dataset\n",
    "Parameter: epochs=30, batch=16\n",
    "\n",
    "## Result\n",
    "number of image: 244\n",
    "\n",
    "Precision: 0.620\n",
    "\n",
    "Recall: 0.739 (Finds just over half od all real trash object)\n",
    "\n",
    "mAP@50: 0.620 \n",
    "\n",
    "mAP@50-95: 0.379\n",
    "\n",
    "## Time used\n",
    "Training: 1398.30 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdd86a5-ed82-4656-bb22-74c074167b4b",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04f6ba60-1db3-422f-8889-b7b5cc5243dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(save_dir, \"fasterrcnn_trained_argumented.pth\")\n",
    "torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64d45e-e195-4325-9d0c-727c62231e03",
   "metadata": {},
   "source": [
    "### Test on my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5739b42d-21a1-4867-9c8d-a64241d7c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peggy\\AppData\\Local\\Temp\\ipykernel_7488\\1850192063.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class       Images   Instances  Box(P)       R   mAP50    mAP50-95\n",
      "all            185         253   0.697   0.885   0.697       0.351\n",
      "Total evaluation time: 58.54s\n",
      "Avg time per image: 0.3164s (316.4 ms)\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_fasterrcnn(\n",
    "img_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\my_trash_dataset\\images\",\n",
    "lbl_dir = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\my_trash_dataset\\labels\",\n",
    "checkpoint_path = r\"C:\\Users\\peggy\\Desktop\\Trash_detection\\Dataset\\models\\fasterrcnn_trained_argumented.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07177624-b8fb-4188-bab9-c7a48d37e448",
   "metadata": {},
   "source": [
    "## Result on my data\n",
    "Precision: 0.697\n",
    "\n",
    "Recall: 0.885\n",
    "\n",
    "mAP50: 0.697\n",
    "\n",
    "mAP50-95: 0.351\n",
    "\n",
    "Speed: Avg inference time/image: 316.4 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6bd67-abcf-487b-8253-6b1adeebd438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yolov8_clean]",
   "language": "python",
   "name": "conda-env-yolov8_clean-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
